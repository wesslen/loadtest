{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview loadtest is a general-purpose HTTP load-testing and benchmarking library. Docs can be found here https://wesslen.github.io/loadtest/ . Setup You can install the package via pip : pip install \"loadtest @ git+https://github.com/wesslen/loadtest\" For the development, run: $ git clone https://github.com/wesslen/loadtest.git $ python3.10 -m venv venv $ source venv/bin/activate (venv) $ pip install -e . This repo was developed using Python 3.10 and Mac/Unix. It has not yet been tested on different Python versions and OS (e.g., Windows). Execute (venv) $ python -m loadtest \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Metric \u2503 Value \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 URL \u2502 https://fireworks.ai/ \u2502 \u2502 Total Requests \u2502 100 \u2502 \u2502 Failed Requests \u2502 0 \u2502 \u2502 Median Latency \u2502 0.3541 seconds \u2502 \u2502 75% Latency \u2502 0.4154 seconds \u2502 \u2502 95% Latency \u2502 0.6565 seconds \u2502 \u2502 99% Latency \u2502 0.7993 seconds \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Since loadtest is a typer function, you may use --help to provide simple docs: (venv) $ python -m loadtest --help Usage: python -m loadtest [OPTIONS] Simple HTTP benchmarking tool. \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 --url TEXT The URL to benchmark. [default: https://fireworks.ai/] \u2502 \u2502 --num-requests INTEGER Total number of requests to perform. [default: 100] \u2502 \u2502 --method TEXT HTTP method to use. [default: GET] \u2502 \u2502 --concurrency INTEGER Number of concurrent requests. [default: 1] \u2502 \u2502 --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. \u2502 \u2502 [default: None] \u2502 \u2502 --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or \u2502 \u2502 customize the installation. \u2502 \u2502 [default: None] \u2502 \u2502 --help Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Tests Next, you may modify the load test matrix by modifying the file loadtest/test_config.json : { \"endpoints\": [ \"https://jsonplaceholder.typicode.com/comments\", \"https://jsonplaceholder.typicode.com/todos\" ], \"request_types\": [\"GET\", \"POST\"], \"payload_sizes\": [100, 500, 1000], \"concurrency_levels\": [1, 10, 50, 100] } By default, I'm using these ranges. You would want to adjust these values to reflect realistic scenarios for the application being tested. I also used jsonplaceholder 's default API's as dummy endpoints. You would want to change these to reflect the endpoint you want to test. # from the root (venv) $ python -m loadtest.run_tests full Executed 1 GET requests to https://jsonplaceholder.typicode.com/comments with payload size 100 bytes in 0.23 seconds. Executed 10 GET requests to https://jsonplaceholder.typicode.com/comments with payload size 100 bytes in 1.71 seconds. ... Alternatively, you may run a fractional by running: # from the root (venv) $ python -m loadtest.run_tests fractional 0.5 Executed 1 GET requests to https://jsonplaceholder.typicode.com/comments with payload size 100 bytes in 0.23 seconds. Executed 50 GET requests to https://jsonplaceholder.typicode.com/comments with payload size 500 bytes in 8.53 seconds. ... This test will print results to console as well as save results as a .csv file in loadtest/data with a timestamped named. (venv) $ python -m loadtest.run_tests --help Usage: python -m loadtest.run_tests [OPTIONS] Executes a series of load tests based on configurations defined in a JSON file, allowing for either full or fractional testing. The script generates a test matrix from the configuration, runs the tests as per the matrix, and saves the results to a specified output directory. \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 --config-path TEXT The path to the configuration JSON file. [default: loadtest/test_config.json] \u2502 \u2502 --data-dir TEXT The directory where the output results will be saved. [default: loadtest/data] \u2502 \u2502 --design-type TEXT The design type of the test matrix: 'full' or 'fractional'. [default: full] \u2502 \u2502 --fraction FLOAT The fraction of the test matrix to use, required only if design_type is \u2502 \u2502 'fractional'. \u2502 \u2502 [default: None] \u2502 \u2502 --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. [default: None] \u2502 \u2502 --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the \u2502 \u2502 installation. \u2502 \u2502 [default: None] \u2502 \u2502 --help Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f Visualizing Tests You may run a streamlit app to visualize the results by running: (venv) $ python -m streamlit run loadtest/visualize_results.py Within the app, you can select options based on: the file (timestamp of the run) endpoint type of request: GET vs. POST y axis metric: Duration (in seconds) or Failures","title":"Overview"},{"location":"#overview","text":"loadtest is a general-purpose HTTP load-testing and benchmarking library. Docs can be found here https://wesslen.github.io/loadtest/ .","title":"Overview"},{"location":"#setup","text":"You can install the package via pip : pip install \"loadtest @ git+https://github.com/wesslen/loadtest\" For the development, run: $ git clone https://github.com/wesslen/loadtest.git $ python3.10 -m venv venv $ source venv/bin/activate (venv) $ pip install -e . This repo was developed using Python 3.10 and Mac/Unix. It has not yet been tested on different Python versions and OS (e.g., Windows).","title":"Setup"},{"location":"#execute","text":"(venv) $ python -m loadtest \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513 \u2503 Metric \u2503 Value \u2503 \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529 \u2502 URL \u2502 https://fireworks.ai/ \u2502 \u2502 Total Requests \u2502 100 \u2502 \u2502 Failed Requests \u2502 0 \u2502 \u2502 Median Latency \u2502 0.3541 seconds \u2502 \u2502 75% Latency \u2502 0.4154 seconds \u2502 \u2502 95% Latency \u2502 0.6565 seconds \u2502 \u2502 99% Latency \u2502 0.7993 seconds \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Since loadtest is a typer function, you may use --help to provide simple docs: (venv) $ python -m loadtest --help Usage: python -m loadtest [OPTIONS] Simple HTTP benchmarking tool. \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 --url TEXT The URL to benchmark. [default: https://fireworks.ai/] \u2502 \u2502 --num-requests INTEGER Total number of requests to perform. [default: 100] \u2502 \u2502 --method TEXT HTTP method to use. [default: GET] \u2502 \u2502 --concurrency INTEGER Number of concurrent requests. [default: 1] \u2502 \u2502 --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. \u2502 \u2502 [default: None] \u2502 \u2502 --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or \u2502 \u2502 customize the installation. \u2502 \u2502 [default: None] \u2502 \u2502 --help Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"Execute"},{"location":"#tests","text":"Next, you may modify the load test matrix by modifying the file loadtest/test_config.json : { \"endpoints\": [ \"https://jsonplaceholder.typicode.com/comments\", \"https://jsonplaceholder.typicode.com/todos\" ], \"request_types\": [\"GET\", \"POST\"], \"payload_sizes\": [100, 500, 1000], \"concurrency_levels\": [1, 10, 50, 100] } By default, I'm using these ranges. You would want to adjust these values to reflect realistic scenarios for the application being tested. I also used jsonplaceholder 's default API's as dummy endpoints. You would want to change these to reflect the endpoint you want to test. # from the root (venv) $ python -m loadtest.run_tests full Executed 1 GET requests to https://jsonplaceholder.typicode.com/comments with payload size 100 bytes in 0.23 seconds. Executed 10 GET requests to https://jsonplaceholder.typicode.com/comments with payload size 100 bytes in 1.71 seconds. ... Alternatively, you may run a fractional by running: # from the root (venv) $ python -m loadtest.run_tests fractional 0.5 Executed 1 GET requests to https://jsonplaceholder.typicode.com/comments with payload size 100 bytes in 0.23 seconds. Executed 50 GET requests to https://jsonplaceholder.typicode.com/comments with payload size 500 bytes in 8.53 seconds. ... This test will print results to console as well as save results as a .csv file in loadtest/data with a timestamped named. (venv) $ python -m loadtest.run_tests --help Usage: python -m loadtest.run_tests [OPTIONS] Executes a series of load tests based on configurations defined in a JSON file, allowing for either full or fractional testing. The script generates a test matrix from the configuration, runs the tests as per the matrix, and saves the results to a specified output directory. \u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e \u2502 --config-path TEXT The path to the configuration JSON file. [default: loadtest/test_config.json] \u2502 \u2502 --data-dir TEXT The directory where the output results will be saved. [default: loadtest/data] \u2502 \u2502 --design-type TEXT The design type of the test matrix: 'full' or 'fractional'. [default: full] \u2502 \u2502 --fraction FLOAT The fraction of the test matrix to use, required only if design_type is \u2502 \u2502 'fractional'. \u2502 \u2502 [default: None] \u2502 \u2502 --install-completion [bash|zsh|fish|powershell|pwsh] Install completion for the specified shell. [default: None] \u2502 \u2502 --show-completion [bash|zsh|fish|powershell|pwsh] Show completion for the specified shell, to copy it or customize the \u2502 \u2502 installation. \u2502 \u2502 [default: None] \u2502 \u2502 --help Show this message and exit. \u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f","title":"Tests"},{"location":"#visualizing-tests","text":"You may run a streamlit app to visualize the results by running: (venv) $ python -m streamlit run loadtest/visualize_results.py Within the app, you can select options based on: the file (timestamp of the run) endpoint type of request: GET vs. POST y axis metric: Duration (in seconds) or Failures","title":"Visualizing Tests"},{"location":"cli/","text":"CLI Reference This page provides documentation for the command line tools. loadtest Executes a simple HTTP benchmarking tool that performs a specified number of HTTP requests to a given URL, displays the results including various latency metrics, and handles concurrent requests. This tool allows users to specify the target URL, the total number of requests, the HTTP method (GET or POST), and the level of concurrency for the requests. It provides a detailed report on the performance of the HTTP requests, including metrics such as median, 75th percentile, 95th percentile, and 99th percentile latencies. Parameters: url (str): The URL to be benchmarked. This is the target endpoint where the HTTP requests will be sent. num_requests (int): The total number of HTTP requests to perform against the target URL. This defines the load intensity of the benchmark. method (str): The HTTP method to use for the requests. This can be either 'GET' or 'POST'. The default method is 'GET'. concurrency (int): The number of concurrent requests to make. This parameter allows the benchmark to simulate multiple users or processes making requests to the same endpoint simultaneously. The benchmark will output a table showing the URL tested, the total number of requests made, the number of failed requests, and latency statistics (median, 75%, 95%, and 99% latencies) to provide a comprehensive view of the endpoint's performance under the specified load. Example usage: python -m loadtest --url http://example.com --num-requests 100 --method GET --concurrency 10 This command will benchmark the specified URL using 100 GET requests with 10 concurrent requests. Note: The tool requires an internet connection to perform the HTTP requests and the URL provided should be accessible from the host machine where the script is run. loadtest.load_tester Executes a load test on a specified endpoint using a specified number of concurrent requests of a given type and, for POST requests, a specified payload size. The function reports on the success and failure of the requests and provides a summary of the test's execution time. Parameters: endpoint (str): The target URL for the load testing. This should be a valid HTTP or HTTPS endpoint where the load test will send the requests. request_type (str): Specifies the type of HTTP request to perform. Valid options are 'GET' or 'POST'. This determines the HTTP method used to interact with the endpoint. payload_size (int): Defines the size of the payload in bytes for POST requests. This is ignored for GET requests. The payload is a string of repeated 'x' characters of the specified size. concurrency (int): The number of requests to send concurrently. This simulates multiple users or systems interacting with the endpoint simultaneously and can help assess the endpoint's capacity to handle multiple requests. The function performs the load test by sending the specified number of concurrent requests to the endpoint. For POST requests, it sends a payload of the specified size with each request. The function counts the number of successful requests and failures and prints a summary of the test results, including the total duration of the test, the number of successful creations (for POST requests), and the number of failures. Example usage: python -m loadtest.load_tester --endpoint http://example.com/api --request-type POST --payload-size 500 --concurrency 10 This command will send 10 concurrent POST requests to the specified endpoint, each with a payload of 500 bytes. Note: This script is intended for load testing purposes and should be used responsibly. Ensure that you have permission to perform load testing on the target endpoint and that the test will not negatively impact the endpoint or its network. Returns: duration (float): The total time taken to execute all the requests in seconds. creations (int): The number of successful resource creations (relevant for POST requests). failures (int): The number of requests that resulted in failure (e.g., due to server errors or timeouts). loadtest.run_tests Executes a series of load tests based on configurations defined in a JSON file, allowing for either full or fractional testing. The script generates a test matrix from the configuration, runs the tests as per the matrix, and saves the results to a CSV file. The test matrix is constructed from a combination of endpoints, request types, payload sizes, and concurrency levels specified in the configuration file. Users can choose to run all possible combinations (full) or a random fraction of these combinations (fractional). Parameters: - config_path (str): The path to the configuration JSON file. By default, it's set to loadtest/test_config.json . - data_dir (str): The directory where the output results will be saved. By default it is loadtest/data . - design_type (str): Specifies the type of test matrix design to use. It can be 'full' for testing all combinations or 'fractional' for testing a fraction of all combinations. - fraction (float): Specifies the fraction of the test matrix to execute. This is required if design_type is 'fractional'. For example, a fraction of 0.5 means that half of the total combinations will be randomly selected and tested. The script performs the following steps: Constructs the test matrix from the configuration file. Depending on the design_type, it either uses the full matrix or selects a fraction of the combinations. Executes the load tests for each combination in the final test matrix using the LoadTester class. Collects the results from each test, including the duration, number of creations, and number of failures. Saves the results in a CSV file with a timestamp in the filename to ensure uniqueness. Results are saved in the 'loadtest/data' directory with filenames following the 'results_YYYYMMDD_HHMMSS.csv' format. Example usage: python -m loadtest.run_tests --design-type fractional --fraction 0.5 This command will execute a load test using a fractional design, testing only half of the possible combinations defined in the test configuration file. Note: This script relies on the LoadTester class for executing the load tests. loadtest.visualize_results Launches a Streamlit app to visualize load test results from CSV files stored in a specified directory. The app provides interactive controls to select a results file, endpoint, request type, and metric for visualization. It displays a line chart and a detailed data table based on the selections. Parameter: data_dir (str): Path to the directory containing the CSV files with load test results. By default it is tests/data The Streamlit app includes: A dropdown to select a results file from the specified directory. A dropdown to select an endpoint from the available options in the selected file. Dropdowns to select the type of request and the metric (Duration or Failures) to display on the y-axis of the chart. An Altair line chart visualizing the selected metric over concurrency levels, colored by payload sizes. A data table displaying the detailed data for the selected filters. To run the app, execute the command: python -m streamlit run loadtest/visualize_results.py --data-dir <path_to_data_dir>","title":"CLI Reference"},{"location":"cli/#cli-reference","text":"This page provides documentation for the command line tools.","title":"CLI Reference"},{"location":"cli/#loadtest","text":"Executes a simple HTTP benchmarking tool that performs a specified number of HTTP requests to a given URL, displays the results including various latency metrics, and handles concurrent requests. This tool allows users to specify the target URL, the total number of requests, the HTTP method (GET or POST), and the level of concurrency for the requests. It provides a detailed report on the performance of the HTTP requests, including metrics such as median, 75th percentile, 95th percentile, and 99th percentile latencies. Parameters: url (str): The URL to be benchmarked. This is the target endpoint where the HTTP requests will be sent. num_requests (int): The total number of HTTP requests to perform against the target URL. This defines the load intensity of the benchmark. method (str): The HTTP method to use for the requests. This can be either 'GET' or 'POST'. The default method is 'GET'. concurrency (int): The number of concurrent requests to make. This parameter allows the benchmark to simulate multiple users or processes making requests to the same endpoint simultaneously. The benchmark will output a table showing the URL tested, the total number of requests made, the number of failed requests, and latency statistics (median, 75%, 95%, and 99% latencies) to provide a comprehensive view of the endpoint's performance under the specified load. Example usage: python -m loadtest --url http://example.com --num-requests 100 --method GET --concurrency 10 This command will benchmark the specified URL using 100 GET requests with 10 concurrent requests. Note: The tool requires an internet connection to perform the HTTP requests and the URL provided should be accessible from the host machine where the script is run.","title":"loadtest"},{"location":"cli/#loadtestload_tester","text":"Executes a load test on a specified endpoint using a specified number of concurrent requests of a given type and, for POST requests, a specified payload size. The function reports on the success and failure of the requests and provides a summary of the test's execution time. Parameters: endpoint (str): The target URL for the load testing. This should be a valid HTTP or HTTPS endpoint where the load test will send the requests. request_type (str): Specifies the type of HTTP request to perform. Valid options are 'GET' or 'POST'. This determines the HTTP method used to interact with the endpoint. payload_size (int): Defines the size of the payload in bytes for POST requests. This is ignored for GET requests. The payload is a string of repeated 'x' characters of the specified size. concurrency (int): The number of requests to send concurrently. This simulates multiple users or systems interacting with the endpoint simultaneously and can help assess the endpoint's capacity to handle multiple requests. The function performs the load test by sending the specified number of concurrent requests to the endpoint. For POST requests, it sends a payload of the specified size with each request. The function counts the number of successful requests and failures and prints a summary of the test results, including the total duration of the test, the number of successful creations (for POST requests), and the number of failures. Example usage: python -m loadtest.load_tester --endpoint http://example.com/api --request-type POST --payload-size 500 --concurrency 10 This command will send 10 concurrent POST requests to the specified endpoint, each with a payload of 500 bytes. Note: This script is intended for load testing purposes and should be used responsibly. Ensure that you have permission to perform load testing on the target endpoint and that the test will not negatively impact the endpoint or its network. Returns: duration (float): The total time taken to execute all the requests in seconds. creations (int): The number of successful resource creations (relevant for POST requests). failures (int): The number of requests that resulted in failure (e.g., due to server errors or timeouts).","title":"loadtest.load_tester"},{"location":"cli/#loadtestrun_tests","text":"Executes a series of load tests based on configurations defined in a JSON file, allowing for either full or fractional testing. The script generates a test matrix from the configuration, runs the tests as per the matrix, and saves the results to a CSV file. The test matrix is constructed from a combination of endpoints, request types, payload sizes, and concurrency levels specified in the configuration file. Users can choose to run all possible combinations (full) or a random fraction of these combinations (fractional). Parameters: - config_path (str): The path to the configuration JSON file. By default, it's set to loadtest/test_config.json . - data_dir (str): The directory where the output results will be saved. By default it is loadtest/data . - design_type (str): Specifies the type of test matrix design to use. It can be 'full' for testing all combinations or 'fractional' for testing a fraction of all combinations. - fraction (float): Specifies the fraction of the test matrix to execute. This is required if design_type is 'fractional'. For example, a fraction of 0.5 means that half of the total combinations will be randomly selected and tested. The script performs the following steps: Constructs the test matrix from the configuration file. Depending on the design_type, it either uses the full matrix or selects a fraction of the combinations. Executes the load tests for each combination in the final test matrix using the LoadTester class. Collects the results from each test, including the duration, number of creations, and number of failures. Saves the results in a CSV file with a timestamp in the filename to ensure uniqueness. Results are saved in the 'loadtest/data' directory with filenames following the 'results_YYYYMMDD_HHMMSS.csv' format. Example usage: python -m loadtest.run_tests --design-type fractional --fraction 0.5 This command will execute a load test using a fractional design, testing only half of the possible combinations defined in the test configuration file. Note: This script relies on the LoadTester class for executing the load tests.","title":"loadtest.run_tests"},{"location":"cli/#loadtestvisualize_results","text":"Launches a Streamlit app to visualize load test results from CSV files stored in a specified directory. The app provides interactive controls to select a results file, endpoint, request type, and metric for visualization. It displays a line chart and a detailed data table based on the selections. Parameter: data_dir (str): Path to the directory containing the CSV files with load test results. By default it is tests/data The Streamlit app includes: A dropdown to select a results file from the specified directory. A dropdown to select an endpoint from the available options in the selected file. Dropdowns to select the type of request and the metric (Duration or Failures) to display on the y-axis of the chart. An Altair line chart visualizing the selected metric over concurrency levels, colored by payload sizes. A data table displaying the detailed data for the selected filters. To run the app, execute the command: python -m streamlit run loadtest/visualize_results.py --data-dir <path_to_data_dir>","title":"loadtest.visualize_results"}]}